from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import chromadb
from chromadb.utils.embedding_functions import OllamaEmbeddingFunction

# 1. Load your .txt file
loader = TextLoader("/media/pope/projecteo/github_proj/SageScript/codedb/reuls.txt", encoding="utf-8")
documents = loader.load()

# 2. Split text into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
texts = text_splitter.split_documents(documents)

# 3. Initialize Chroma client
client = chromadb.Client()

# 4. Setup Ollama embedding function pointing to your local Ollama server
ollama_ef = OllamaEmbeddingFunction(
    model_name="nomic-embed-text",  # or another Ollama embedding model you prefer
    url="http://localhost:11434/api/embeddings"
)

# 5. Create a collection in ChromaDB using Ollama embedding function
collection = client.create_collection(name="my_text_collection", embedding_function=ollama_ef)

# 6. Add documents with embeddings generated by Ollama
for i, doc in enumerate(texts):
    collection.add(
        documents=[doc.page_content],
        ids=[str(i)],
        metadatas=[{"source": "your_file.txt"}]
    )

print("Text file successfully converted and stored in ChromaDB vector database using Ollama embeddings.")
